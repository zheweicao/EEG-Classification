{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pywt\n",
    "\n",
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowLength = 4 # sample window length, in secs\n",
    "stride = 0.1  # moving window stride, in secs\n",
    "overlap = windowLength - stride\n",
    "ogFs = 5000 # original sampling rate, in Hz\n",
    "dsFs = 200 # sampling rate, in Hz\n",
    "nSamples = int(dsFs * windowLength)\n",
    "nStride = int(dsFs * stride)\n",
    "nOverlap = int(dsFs * overlap)\n",
    "fft = True\n",
    "wavelet = False\n",
    "if fft:\n",
    "    nFft = 64 # FFT bin size\n",
    "else:\n",
    "    nFft = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a label encoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Specify the directory path\n",
    "directory = '/Users/zhewei/Library/CloudStorage/GoogleDrive-zhewei@umich.edu/My Drive/BYB_EEG_Classfication/Data_Trimmed_200Hz_20s/bundled'\n",
    "\n",
    "# Read all CSV files in the directory\n",
    "dataframes = []\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        df = pd.read_csv(filepath)\n",
    "        # Fit the encoder to the string column and  transform it to numerical data\n",
    "        df['EyesLabel'] = le.fit_transform(df['EyesLabel'])\n",
    "        # Normalize the EEG data\n",
    "        df['EEGdata']= zscore(df['EEGdata'])\n",
    "        dataframes.append(df)\n",
    "\n",
    "        # Concatenate all dataframes into a single dataframe\n",
    "        combined_df = pd.concat(dataframes)\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "combined_df = pd.concat(dataframes)\n",
    "\n",
    "# extract the spectral features from the EEG data\n",
    "# fft 64 points\n",
    "feature_df = pd.DataFrame()\n",
    "\n",
    "for i in range(0, len(combined_df), nSamples - nOverlap):\n",
    "    # Get the current window\n",
    "    windowData = combined_df['EEGdata'].iloc[i:i + nSamples]\n",
    "    wavelet_transform = pywt.wavedec(windowData, 'haar', level=2)\n",
    "\n",
    "    # Check if the window is of the correct length\n",
    "    if len(windowData) < nSamples:\n",
    "        break\n",
    "    # Compute the spectrogram\n",
    "    else:\n",
    "        fft_result = np.fft.fft(windowData, n=nFft)\n",
    "        fft_result = np.abs(fft_result) \n",
    "        # now that every window has 64 spectral features\n",
    "        # we want to give each of them a column name\n",
    "        feature_df.loc[i, 'EyesLabel'] = combined_df['EyesLabel'].iloc[i]\n",
    "        for j in range(nFft):\n",
    "            feature_df.loc[i, f'fft_{j}'] = fft_result[j]\n",
    "\n",
    "# zero pad the feature_df\n",
    "feature_df = feature_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (9958, 65)\n",
      "Test set shape: (3113, 65)\n",
      "Validation set shape: (2490, 65)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train and test sets\n",
    "train_df, test_df = train_test_split(feature_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the train set into train and validation sets\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the train, test, and validation sets\n",
    "print(\"Train set shape:\", train_df.shape)\n",
    "print(\"Test set shape:\", test_df.shape)\n",
    "print(\"Validation set shape:\", val_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (2079, 65)\n",
      "Validation set shape: (2074, 65)\n",
      "Train set shape: (4153, 65)\n",
      "Validation set shape: (2074, 65)\n",
      "Train set shape: (6227, 65)\n",
      "Validation set shape: (2074, 65)\n",
      "Train set shape: (8301, 65)\n",
      "Validation set shape: (2074, 65)\n",
      "Train set shape: (10375, 65)\n",
      "Validation set shape: (2074, 65)\n",
      "Test set shape: (3112, 65)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import numpy as np\n",
    "\n",
    "# Number of splits\n",
    "n_splits = 5\n",
    "\n",
    "# Initialize TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Assume 'feature_df' is your entire dataset\n",
    "# Reserve last 20% of data as the test set\n",
    "test_size = int(len(feature_df) * 0.2)\n",
    "train_val_df = feature_df[:-test_size]\n",
    "test_df = feature_df[-test_size:]\n",
    "\n",
    "# Use TimeSeriesSplit to create train/validation sets\n",
    "for train_index, val_index in tscv.split(train_val_df):\n",
    "    train_df = train_val_df.iloc[train_index]\n",
    "    val_df = train_val_df.iloc[val_index]\n",
    "    print(f\"Train set shape: {train_df.shape}\")\n",
    "    print(f\"Validation set shape: {val_df.shape}\")\n",
    "\n",
    "# Now print the test set shape\n",
    "print(\"Test set shape:\", test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "train_features = torch.tensor(train_df.iloc[:, 1:].values.astype(np.float32))  # all columns except the first one\n",
    "train_labels = torch.tensor(train_df.iloc[:, 0].values.astype(np.float32))  # only the first column\n",
    "train_dataset = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=False, num_workers=4,persistent_workers=True)\n",
    "\n",
    "\n",
    "#  Validate Set\n",
    "val_features = torch.tensor(val_df.iloc[:, 1:].values.astype(np.float32))\n",
    "val_labels = torch.tensor(val_df.iloc[:, 0].values.astype(np.float32))\n",
    "val_dataset = torch.utils.data.TensorDataset(val_features, val_labels)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4,persistent_workers=True)\n",
    "\n",
    "\n",
    "# test\n",
    "test_features = torch.tensor(test_df.iloc[:, 1:].values.astype(np.float32))  # all columns except the first one\n",
    "test_labels = torch.tensor(test_df.iloc[:, 0].values.astype(np.float32))  # only the first column\n",
    "test_dataset = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4,persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUNetwork(\n",
      "  (gru): GRU(64, 50, batch_first=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=50, out_features=32, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Epoch 1: Accuracy: 0.5407986044883728, F1 Score: 0.5727362632751465\n",
      "Epoch 1: Val Accuracy: 0.58349609375, Val F1 Score: 0.6207203269004822\n",
      "Epoch 2: Accuracy: 0.6184413433074951, F1 Score: 0.6427023410797119\n",
      "Epoch 2: Val Accuracy: 0.64208984375, Val F1 Score: 0.6639156341552734\n",
      "Epoch 3: Accuracy: 0.6656057238578796, F1 Score: 0.6791300177574158\n",
      "Epoch 3: Val Accuracy: 0.66748046875, Val F1 Score: 0.615905225276947\n",
      "Epoch 4: Accuracy: 0.6923225522041321, F1 Score: 0.6980310678482056\n",
      "Epoch 4: Val Accuracy: 0.70703125, Val F1 Score: 0.7047244310379028\n",
      "Epoch 5: Accuracy: 0.7227044701576233, F1 Score: 0.7331786751747131\n",
      "Epoch 5: Val Accuracy: 0.671875, Val F1 Score: 0.718120813369751\n",
      "Epoch 6: Accuracy: 0.7594521641731262, F1 Score: 0.7722790241241455\n",
      "Epoch 6: Val Accuracy: 0.650390625, Val F1 Score: 0.5803048014640808\n",
      "Epoch 7: Accuracy: 0.7741126418113708, F1 Score: 0.7799285650253296\n",
      "Epoch 7: Val Accuracy: 0.72705078125, Val F1 Score: 0.741801381111145\n",
      "Epoch 8: Accuracy: 0.7891589403152466, F1 Score: 0.796613335609436\n",
      "Epoch 8: Val Accuracy: 0.650390625, Val F1 Score: 0.710590124130249\n",
      "Epoch 9: Accuracy: 0.8054590821266174, F1 Score: 0.8119347095489502\n",
      "Epoch 9: Val Accuracy: 0.73095703125, Val F1 Score: 0.7428838014602661\n",
      "Epoch 10: Accuracy: 0.8100887537002563, F1 Score: 0.8142628073692322\n",
      "Epoch 10: Val Accuracy: 0.65283203125, Val F1 Score: 0.7001265287399292\n",
      "Test Accuracy: 0.6620489954948425, Test F1 Score: 0.6910161972045898\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "import torchmetrics\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "# Step 1: Define the GRU network architecture\n",
    "class GRUNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size,dropout=0.5):\n",
    "        super(GRUNetwork, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, hidden = self.gru(x)\n",
    "        last_hidden_state = hidden[-1]  \n",
    "        output = self.fc(last_hidden_state)\n",
    "        output = self.sigmoid(output)\n",
    "        return output\n",
    "\n",
    "# Define the input size, hidden size, and output size\n",
    "input_size = nFft  # Assuming one feature (EEGData) as input\n",
    "hidden_size = 50  # Number of hidden units in the GRU layer\n",
    "batch_size = 32  # Number of samples in each batch\n",
    "output_size = batch_size  # Number of output units (labels)\n",
    "\n",
    "# Create an instance of the GRU network\n",
    "gru_net = GRUNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# Print the network architecture\n",
    "print(gru_net)\n",
    "\n",
    "# Step 2: Prepare the data\n",
    "# Create PyTorch DataLoader objects for each set\n",
    "\n",
    "# Create a DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=4,persistent_workers=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4,persistent_workers=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4,persistent_workers=True)\n",
    "\n",
    "# Step 3: Define the training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "model = GRUNetwork(input_size, hidden_size, output_size).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate,weight_decay=1e-5)\n",
    "\n",
    "# Metrics\n",
    "accuracy = torchmetrics.Accuracy(task='binary',threshold=0.5).to(device)\n",
    "f1_score = torchmetrics.F1Score(task='binary',threshold=0.5).to(device)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_accuracy = 0.0\n",
    "\n",
    "    for inputs,labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        if len(labels) != batch_size or len(inputs) != batch_size:\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update metrics\n",
    "        accuracy.update(outputs, labels.float())\n",
    "        f1_score.update(outputs, labels.float())\n",
    "\n",
    "    # Log epoch metrics\n",
    "    train_acc = accuracy.compute()\n",
    "    train_f1 = f1_score.compute()\n",
    "    print(f\"Epoch {epoch+1}: Accuracy: {train_acc}, F1 Score: {train_f1}\")\n",
    "\n",
    "    # Reset metrics for next epoch\n",
    "    accuracy.reset()\n",
    "    f1_score.reset()\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    val_accuracy = 0.0\n",
    "\n",
    "    class EarlyStopping:\n",
    "        def __init__(self, patience=7, min_delta=0):\n",
    "            self.patience = patience\n",
    "            self.min_delta = min_delta\n",
    "            self.counter = 0\n",
    "            self.best_score = None\n",
    "            self.early_stop = False\n",
    "\n",
    "        def __call__(self, val_acc, model):\n",
    "            score = val_acc\n",
    "\n",
    "            if self.best_score is None:\n",
    "                self.best_score = score\n",
    "            elif score < self.best_score + self.min_delta:\n",
    "                self.counter += 1\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "                if self.counter >= self.patience:\n",
    "                    self.early_stop = True\n",
    "            else:\n",
    "                self.best_score = score\n",
    "                self.counter = 0\n",
    "\n",
    "    # Initialize early stopping\n",
    "    early_stopping = EarlyStopping(patience=10, min_delta=0.01)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            if len(labels) != batch_size or len(inputs) != batch_size:\n",
    "                continue\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.float())\n",
    "\n",
    "            # Update validation metrics\n",
    "            accuracy.update(outputs, labels.float())\n",
    "            f1_score.update(outputs, labels.float())\n",
    "\n",
    "        # Log validation metrics\n",
    "        val_acc = accuracy.compute()\n",
    "        val_f1 = f1_score.compute()\n",
    "        # implement the early stopping callback\n",
    "         \n",
    "        print(f\"Epoch {epoch+1}: Val Accuracy: {val_acc}, Val F1 Score: {val_f1}\")\n",
    "\n",
    "\n",
    "        accuracy.reset()\n",
    "        f1_score.reset()\n",
    "\n",
    "        # Call early stopping\n",
    "        early_stopping(val_acc, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "    # Implement early stopping or save the best model based on validation accuracy here\n",
    "\n",
    "# Evaluation loop\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        if len(labels) != batch_size or len(inputs) != batch_size:\n",
    "            continue\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        # Update metrics\n",
    "        accuracy.update(outputs, labels.float())\n",
    "        f1_score.update(outputs, labels.float())\n",
    "\n",
    "test_acc = accuracy.compute()\n",
    "test_f1 = f1_score.compute()\n",
    "print(f\"Test Accuracy: {test_acc}, Test F1 Score: {test_f1}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "import torchmetrics\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "# Step 1: Define the GRU network architecture\n",
    "class GRUNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size,dropout=0.5):\n",
    "        super(GRUNetwork, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, hidden = self.gru(x)\n",
    "        last_hidden_state = hidden[-1]  \n",
    "        output = self.fc(last_hidden_state)\n",
    "        output = self.sigmoid(output)\n",
    "        return output\n",
    "\n",
    "# Define the input size, hidden size, and output size\n",
    "input_size = nFft  # Assuming one feature (EEGData) as input\n",
    "hidden_size = 50  # Number of hidden units in the GRU layer\n",
    "batch_size = 32  # Number of samples in each batch\n",
    "output_size = batch_size  # Number of output units (labels)\n",
    "\n",
    "# Create an instance of the GRU network\n",
    "gru_net = GRUNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# Print the network architecture\n",
    "print(gru_net)\n",
    "\n",
    "# Step 2: Prepare the data\n",
    "# Create PyTorch DataLoader objects for each set\n",
    "\n",
    "# Create a DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=4,persistent_workers=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4,persistent_workers=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4,persistent_workers=True)\n",
    "\n",
    "# Step 3: Define the training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "model = GRUNetwork(input_size, hidden_size, output_size).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate,weight_decay=1e-5)\n",
    "\n",
    "# Metrics\n",
    "accuracy = torchmetrics.Accuracy(task='binary',threshold=0.5).to(device)\n",
    "f1_score = torchmetrics.F1Score(task='binary',threshold=0.5).to(device)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_accuracy = 0.0\n",
    "\n",
    "    for inputs,labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        if len(labels) != batch_size or len(inputs) != batch_size:\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update metrics\n",
    "        accuracy.update(outputs, labels.float())\n",
    "        f1_score.update(outputs, labels.float())\n",
    "\n",
    "    # Log epoch metrics\n",
    "    train_acc = accuracy.compute()\n",
    "    train_f1 = f1_score.compute()\n",
    "    print(f\"Epoch {epoch+1}: Accuracy: {train_acc}, F1 Score: {train_f1}\")\n",
    "\n",
    "    # Reset metrics for next epoch\n",
    "    accuracy.reset()\n",
    "    f1_score.reset()\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    val_accuracy = 0.0\n",
    "\n",
    "    # Define the early stopping callback\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor='val_acc',\n",
    "        min_delta=0.00,\n",
    "        patience=10,\n",
    "        verbose=False,\n",
    "        mode='max'  # Change to 'max' because we want to maximize accuracy\n",
    "        )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            if len(labels) != batch_size or len(inputs) != batch_size:\n",
    "                continue\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.float())\n",
    "\n",
    "            # Update validation metrics\n",
    "            accuracy.update(outputs, labels.float())\n",
    "            f1_score.update(outputs, labels.float())\n",
    "\n",
    "        # Log validation metrics\n",
    "        val_acc = accuracy.compute()\n",
    "        val_f1 = f1_score.compute()\n",
    "        # implement the early stopping callback\n",
    "        early_stop_callback.on_validation_end(pl_module=model, val_acc)\n",
    "\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Val Accuracy: {val_acc}, Val F1 Score: {val_f1}\")\n",
    "\n",
    "\n",
    "        accuracy.reset()\n",
    "        f1_score.reset()\n",
    "\n",
    "    # Implement early stopping or save the best model based on validation accuracy here\n",
    "\n",
    "# Evaluation loop\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        if len(labels) != batch_size or len(inputs) != batch_size:\n",
    "            continue\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        # Update metrics\n",
    "        accuracy.update(outputs, labels.float())\n",
    "        f1_score.update(outputs, labels.float())\n",
    "\n",
    "test_acc = accuracy.compute()\n",
    "test_f1 = f1_score.compute()\n",
    "print(f\"Test Accuracy: {test_acc}, Test F1 Score: {test_f1}\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualize the above GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "               GRU-1  [[-1, 32, 50], [-1, 2, 50]]               0\n",
      "            Linear-2                   [-1, 32]           1,632\n",
      "           Sigmoid-3                   [-1, 32]               0\n",
      "================================================================\n",
      "Total params: 1,632\n",
      "Trainable params: 1,632\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.22\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 1.23\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# visualize the Gated Recurrent Unit (GRU) network architecture\n",
    "from torchsummary import summary\n",
    "summary(model, input_size=(batch_size, nFft), device=device.type)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
